{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-04T09:37:34.623238Z",
     "start_time": "2024-06-04T09:37:34.545241Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def read_model_output(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        model_output_text = file.read()\n",
    "    return model_output_text\n",
    "\n",
    "# Specify the path to your model's output text file\n",
    "file_path = 'predictions_output2.txt'\n",
    "\n",
    "# Load the model output text\n",
    "model_output_text = read_model_output(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T09:37:35.388741Z",
     "start_time": "2024-06-04T09:37:35.369237Z"
    }
   },
   "id": "77eb22859759f727",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def parse_conll(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        token_labels = [line.strip().split() for line in file if line.strip()]\n",
    "    return {token: label for token, label in token_labels if label}\n",
    "\n",
    "def parse_predictions(model_output_text):\n",
    "    predictions = {}\n",
    "    # Split the text block into segments for each \"Text\"\n",
    "    segments = model_output_text.strip().split('Text:')\n",
    "    for segment in segments[1:]:  # Skip the first split as it will be empty\n",
    "        lines = segment.strip().split('\\n')\n",
    "        text = lines[0].strip()  # The first line after 'Text:' is the actual text\n",
    "        # The following lines contain predictions\n",
    "        for line in lines[1:]:\n",
    "            if line.startswith('{'):\n",
    "                # Evaluate the line as a dictionary and merge into predictions\n",
    "                line_dict = eval(line)\n",
    "                predictions.update(line_dict)\n",
    "    return predictions\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T09:37:36.285875Z",
     "start_time": "2024-06-04T09:37:36.267876Z"
    }
   },
   "id": "dbff3631e7d8b61d",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No predictions were made for the following labels: {';', '...'}\n",
      "Precision: 0.8476520145643108\n",
      "Recall: 0.8621574186580262\n",
      "F1 Score: 0.8532131870749726\n",
      "Label: ! - Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "Label: , - Precision: 0.6521, Recall: 0.5040, F1-Score: 0.5686\n",
      "Label: - - Precision: 0.3333, Recall: 0.2247, F1-Score: 0.2685\n",
      "Label: . - Precision: 0.6768, Recall: 0.6282, F1-Score: 0.6516\n",
      "Label: ... - Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "Label: : - Precision: 0.4348, Recall: 0.2128, F1-Score: 0.2857\n",
      "Label: ; - Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000\n",
      "Label: ? - Precision: 0.3810, Recall: 0.3200, F1-Score: 0.3478\n",
      "Label: B - Precision: 0.9105, Recall: 0.9548, F1-Score: 0.9321\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have file paths or string contents for both\n",
    "gold_labels = parse_conll('test_d.conll')\n",
    "predicted_labels = parse_predictions(model_output_text)\n",
    "\n",
    "y_true = [gold_labels[token] for token in gold_labels if token in predicted_labels]\n",
    "y_pred = [predicted_labels[token] for token in gold_labels if token in predicted_labels]\n",
    "\n",
    "# Check which labels were never predicted\n",
    "all_labels = set(y_true)\n",
    "predicted_labels = set(y_pred)\n",
    "unpredicted_labels = all_labels.difference(predicted_labels)\n",
    "\n",
    "if unpredicted_labels:\n",
    "    print(f\"Warning: No predictions were made for the following labels: {unpredicted_labels}\")\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "labels = sorted(set(y_true + y_pred))  # Combine and sort labels to ensure consistent order\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, labels=labels, zero_division=0)\n",
    "\n",
    "# Print metrics for each label\n",
    "for label, p, r, f in zip(labels, precision, recall, f1):\n",
    "    print(f\"Label: {label} - Precision: {p:.4f}, Recall: {r:.4f}, F1-Score: {f:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T09:37:40.922985Z",
     "start_time": "2024-06-04T09:37:37.849043Z"
    }
   },
   "id": "57dbb05291bddaef",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2ed9fa0e7336749"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
