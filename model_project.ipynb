{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77eb00d5d488bd0c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "class InferenceModel:\n",
    "    def __init__(self, model_path, batch_size=1, device=\"cpu\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(model_path).to(device)\n",
    "        self.model.eval()\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "    def prepare_examples(self, texts):\n",
    "        # Tokenize the texts and prepare the dataset\n",
    "        encodings = self.tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        return TextDataset(encodings)\n",
    "\n",
    "    def predict(self, texts):\n",
    "        dataset = self.prepare_examples(texts)\n",
    "        predictions = []\n",
    "        data_loader = DataLoader(dataset, batch_size=self.batch_size)\n",
    "\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**batch)\n",
    "                logits = outputs.logits\n",
    "                predictions.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "760913e53f265c51",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configuration variables\n",
    "model_path = \"./a_model_dir_dkleczek/bert-base-polish-cased-v1_100\"\n",
    "input_file = \"in_c.tsv\"\n",
    "output_file = \"out_c_00epochs.tsv\"\n",
    "\n",
    "# Instantiate model\n",
    "model = InferenceModel(model_path)\n",
    "\n",
    "# Reading and processing input\n",
    "with open(input_file, 'r') as file:\n",
    "    texts = [line.strip().split('\\t')[1] for line in file if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17995707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/tmp/ipykernel_22196/1398453118.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "# Predicting\n",
    "predictions = model.predict(texts)\n",
    "\n",
    "# Writing predictions to output file\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    for text, prediction in zip(texts, predictions):\n",
    "        file.write(f\"Text: {text}\\n\")\n",
    "        file.write(\"Predictions:\\n\")\n",
    "        for token, label in zip(text.split(), prediction):\n",
    "            file.write(f\"{token}: {label}\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c79df7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: \"0\",  \n",
    "    1: \":\",  \n",
    "    2: \";\",\n",
    "    3: \",\",\n",
    "    4: \".\",\n",
    "    5: \"-\",\n",
    "    6: \"...\",\n",
    "    7: \"?\",\n",
    "    8: \"!\"\n",
    "}\n",
    "\n",
    "def process_file(input_file_path, output_file_path):\n",
    "    # Read the input file\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Prepare to write to the output file\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            if lines[i].startswith(\"Text:\"):\n",
    "                file.write(lines[i])  # Write the text line as is\n",
    "                i += 1\n",
    "                file.write(lines[i])  # Write the \"Predictions:\" line as is\n",
    "                i += 1\n",
    "                while i < len(lines) and ':' in lines[i]:  # Process each prediction line\n",
    "                    parts = lines[i].strip().split(': ')\n",
    "                    if parts[1].isdigit():  # Ensure it's a digit to avoid errors\n",
    "                        label = label_map.get(int(parts[1]), \"B\")  # Default to \"B\" if not found\n",
    "                    else:\n",
    "                        label = \"B\"  # Default if not a digit\n",
    "                    file.write(f\"{parts[0]}: {label}\\n\")\n",
    "                    i += 1\n",
    "                file.write(\"\\n\")  # Add a newline after each block of predictions\n",
    "            else:\n",
    "                i += 1  # Skip any lines that don't start a new text block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd7a6363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_file_path = output_file\n",
    "output_file_path = 'cleaned_output_c_100epochs.tsv'\n",
    "process_file(input_file_path, output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
